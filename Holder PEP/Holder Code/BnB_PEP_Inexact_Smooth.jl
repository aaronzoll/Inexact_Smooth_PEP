## Load the packages:
# -------------------
using JuMP, MosekTools, Mosek, LinearAlgebra, OffsetArrays, Gurobi, Ipopt, JLD2, Distributions, OrderedCollections, BenchmarkTools

## Load the pivoted Cholesky finder
# ---------------------------------
include("code_to_compute_pivoted_cholesky.jl")


## Some helper functions

# construct e_i in R^n
function e_i(n, i)
    e_i_vec = zeros(n, 1)
    e_i_vec[i] = 1
    return e_i_vec
end

# this symmetric outer product is used when a is constant, b is a JuMP variable
function âŠ™(a, b)
    return ((a * b') .+ transpose(a * b')) ./ 2
end

# this symmetric outer product is for computing âŠ™(a,a) where a is a JuMP variable
function âŠ™(a)
    return a * transpose(a)
end

# function to compute cardinality of a vector
function compute_cardinality(x, Ïµ_sparsity)
    n = length(x)
    card_x = 0
    for i in 1:n
        if abs(x[i]) >= Ïµ_sparsity
            card_x = card_x + 1
        end
    end
    return card_x
end

# function to compute rank of a matrix
function compute_rank(X, Ïµ_sparsity)
    eigval_array_X = eigvals(X)
    rnk_X = 0
    n = length(eigval_array_X)
    for i in 1:n
        if abs(eigval_array_X[i]) >= Ïµ_sparsity
            rnk_X = rnk_X + 1
        end
    end
    return rnk_X
end


##########################
##  Generators of Data  ##
##########################

# Options for these function are
# step_size_type = :Default => will create a last step of 1/(L) rest will be zero
# step_size_type = :Random => will create a random stepsize

function feasible_h_generator(N, L; step_size_type=:Default)

    # construct h
    # -----------
    h = OffsetArray(zeros(N), 0:N-1)

    if step_size_type == :Default
        for i in 0:N-1
            h[i] = 1 # because we have defined h[i]/L in the FSFOM, h[i]=1 will correspond to a stepsize of 1/L
        end
    elseif step_size_type == :Random
        for i in 0:N-1
            h[i] = Uniform(0, 1) # same
        end
    end

    return h

end


function feasible_H_generator(N, L; step_size_type=:Default)

    # construct H
    # -----------
    H = OffsetArray(zeros(N, N), 1:N, 0:N-1)
    if step_size_type == :Default
        for i in 1:N
            H[i, i-1] = 1 # because we have defined h[i,i-1]/L in the algorithm, so declaring 1 will make the stepsizes equal to 1/L
        end
    elseif step_size_type == :Random
        for i in 1:N
            H[i, i-1] = Uniform(0, 1)
        end
    end

    # find Î± from h
    # -------------


    return H
end


function data_generator_function(N, L, Î±; input_type=:stepsize_constant)

    # define all the bold vectors
    # --------------------------

    # define ð±_0 and ð±_star

    ð±_0 = e_i(N + 2, 1)

    ð±_star = zeros(N + 2, 1)

    # define ð _0, ð _1, â€¦, ð _N

    # first we define the ð  vectors,
    # index -1 corresponds to â‹†, i.e.,  ðŸ[:,-1] =  ðŸ_â‹† = 0

    # ð = [ð _â‹† ð _0 ð _1 ð _2 ... ð _N]
    ð  = OffsetArray(zeros(N + 2, N + 2), 1:N+2, -1:N)
    for i in 0:N
        ð [:, i] = e_i(N + 2, i + 2)
    end

    # time to define the ðŸ vectors

    # ðŸ = [ðŸ_â‹† ðŸ_0, ðŸ_1, â€¦, ðŸ_N]

    ðŸ = OffsetArray(zeros(N + 1, N + 2), 1:N+1, -1:N)

    for i in 0:N
        ðŸ[:, i] = e_i(N + 1, i + 1)
    end

    if input_type == :stepsize_constant

        # ð± = [ð±_{-1}=ð±_â‹† âˆ£ ð±_{0} âˆ£ ð±_{1} âˆ£ â€¦ ð±_{N}] âˆˆ ð‘^(N+2 Ã— N+2)
        ð± = OffsetArray(zeros(N + 2, N + 2), 1:N+2, -1:N)

        # assign values next using our formula for ð±_k
        ð±[:, 0] = ð±_0

        for i in 1:N
            ð±[:, i] = ð±[:, 0] - (1 / L) .* (sum(Î±[i, j] * ð [:, j] for j in 0:i-1))
        end

    elseif input_type == :stepsize_variable

        ð± = [ð±_star ð±_0]

        # assign values next using our formula for ð±_k
        for k in 1:N
            ð±_k = ð±_0 - (1 / L) * (sum(h[i] .* ð [:, i] for i in 0:k-1))
            ð± = [ð± ð±_k]
        end

        # make ð± an offset array to make our life comfortable
        ð± = OffsetArray(ð±, 1:N+2, -1:N)



    end

    return ð±, ð , ðŸ

end


# Index set creator function for the dual variables Î»

struct i_j_m_idx # correspond to (i,j) tuple, where i,j âˆˆ I_N_â‹† 
    i::Int64 # corresponds to index i
    j::Int64 # corresponds to index j
    m::Int64
end

# We have dual variable Î»={Î»_ij}_{i,j} where i,j âˆˆ I_N_star
# The following function creates the maximal index set for Î»

function index_set_constructor_for_dual_vars_full(I_N_star, M)

    # construct the index set for Î»
    idx_set_Î» = i_j_m_idx[] 
    for m in 1:M
        for i in I_N_star
            for j in I_N_star
                if i != j
                    push!(idx_set_Î», i_j_m_idx(i, j, m))
                end
            end
        end
    end
    return idx_set_Î»

end

# The following function will return the effective index set of a known Î» i.e., those indices of  that are  Î»  that are non-zero.

function effective_index_set_finder(Î»; Ïµ_tol=0.0005)

    # the variables Î» are of the type DenseAxisArray whose index set can be accessed using _.axes and data via _.data syntax

    idx_set_Î»_current = (Î».axes)[1]

    idx_set_Î»_effective = i_j_idx[]

    # construct idx_set_Î»_effective

    for i_j_Î» in idx_set_Î»_current
        if abs(Î»[i_j_Î»]) >= Ïµ_tol # if Î»[i,j] >= Ïµ, where Ïµ is our cut off for accepting nonzero
            push!(idx_set_Î»_effective, i_j_Î»)
        end
    end

    return idx_set_Î»_effective

end

# The following function will return the zero index set of a known L_cholesky i.e., those indices of  that are  L  that are zero. ðŸ’€ Note that for Î» we are doing the opposite.

function zero_index_set_finder_L_cholesky(L_cholesky; Ïµ_tol=1e-4)
    n_L_cholesky, _ = size(L_cholesky)
    zero_idx_set_L_cholesky = []
    for i in 1:n_L_cholesky
        for j in 1:n_L_cholesky
            if i >= j # because i<j has L_cholesky[i,j] == 0 for lower-triangual structure
                if abs(L_cholesky[i, j]) <= Ïµ_tol
                    push!(zero_idx_set_L_cholesky, (i, j))
                end
            end
        end
    end
    return zero_idx_set_L_cholesky
end


A_mat(i, j, h, ð , ð±) = âŠ™(ð [:, j], ð±[:, i] - ð±[:, j])
B_mat(i, j, h, ð±) = âŠ™(ð±[:, i] - ð±[:, j], ð±[:, i] - ð±[:, j])
C_mat(i, j, ð ) = âŠ™(ð [:, i] - ð [:, j], ð [:, i] - ð [:, j])
a_vec(i, j, ðŸ) = ðŸ[:, j] - ðŸ[:, i]




##################
##  Optimizers  ##
##################

# step_size options, scaled sets algorithm to use L_epsilon
# otherwise, stepsizes are 1/L

function solve_primal_with_known_stepsizes(N, L, h, R, Îµ, p; show_output=:off, step_size=:scaled)
    # data generator
    # --------------
    L_eps = ((1 - p) / (1 + p) * 1 / Îµ)^((1 - p) / (1 + p)) * (L)^(2 / (1 + p))
    if step_size == :scaled
        ð±, ð , ðŸ = data_generator_function(N, L_eps, h; input_type=:stepsize_constant)
    else
        ð±, ð , ðŸ = data_generator_function(N, L, h; input_type=:stepsize_constant)
    end
    # number of points etc
    # --------------------

    I_N_star = -1:N
    dim_G = N + 2
    dim_Ft = N + 1


    # define the model
    # ----------------

    model_primal_PEP_with_known_stepsizes = Model(optimizer_with_attributes(Mosek.Optimizer))

    # add the variables
    # -----------------

    # construct G âª° 0
    @variable(model_primal_PEP_with_known_stepsizes, G[1:dim_G, 1:dim_G], PSD)

    # construct Ft (this is just transpose of F)
    @variable(model_primal_PEP_with_known_stepsizes, Ft[1:dim_Ft])

    # define objective
    # ----------------

    @objective(model_primal_PEP_with_known_stepsizes, Max,
        Ft' * a_vec(-1, N, ðŸ)
    )

    # interpolation constraint
    # ------------------------

    for i in I_N_star, j in I_N_star
        if i != j
            @constraint(model_primal_PEP_with_known_stepsizes, Ft' * a_vec(i, j, ðŸ) + tr(G * A_mat(i, j, h, ð , ð±)) + ((1 / (2 * (L_eps))) * tr(G * C_mat(i, j, ð ))) - Îµ <= 0
            )
        end
    end


    # initial condition
    # -----------------

    @constraint(model_primal_PEP_with_known_stepsizes, tr(G * B_mat(0, -1, h, ð±)) <= R^2)

    # time to optimize
    # ----------------

    if show_output == :off
        set_silent(model_primal_PEP_with_known_stepsizes)
    end

    optimize!(model_primal_PEP_with_known_stepsizes)

    # store and return the solution
    # -----------------------------

    if termination_status(model_primal_PEP_with_known_stepsizes) != MOI.OPTIMAL
        # @warn "model_primal_PEP_with_known_stepsizes solving did not reach optimality;  termination status = " termination_status(model_primal_PEP_with_known_stepsizes)
    end

    p_star = objective_value(model_primal_PEP_with_known_stepsizes)

    G_star = value.(G)

    Ft_star = value.(Ft)

    return p_star, G_star, Ft_star

end


# Following function uses batch of Îµ to set inexact constraints for set
function solve_primal_with_known_stepsizes_batch(N, L, Î±, R, Îµ_set, p, Î¼; show_output=:off)
    # number of points etc
    # --------------------

    I_N_star = -1:N
    dim_G = N + 2
    dim_Ft = N + 1


    # define the model
    # ----------------

    model_primal_PEP_with_known_stepsizes = Model(optimizer_with_attributes(Mosek.Optimizer))

    # add the variables
    # -----------------

    # construct G âª° 0
    @variable(model_primal_PEP_with_known_stepsizes, G[1:dim_G, 1:dim_G], PSD)

    # construct Ft (this is just transpose of F)
    @variable(model_primal_PEP_with_known_stepsizes, Ft[1:dim_Ft])

    # pick stepsize
    Îµ_opt = maximum(Îµ_set)
    L_step = L

    # define objective
    # ----------------
    #  Î± = compute_Î±_from_h(H, N, Î¼, L)
    ð±, ð , ðŸ = data_generator_function(N, L, Î±; input_type=:stepsize_constant)
    @objective(model_primal_PEP_with_known_stepsizes, Max,
        Ft' * a_vec(-1, N, ðŸ)
    )
    # data generator
    # --------------
    sparsity_pattern = "none"

    if sparsity_pattern == "none"
        for Îµ in Îµ_set
            if p < 1
                L_eps = ((1 - p) / (1 + p) * 1 / Îµ)^((1 - p) / (1 + p)) * (L)^(2 / (1 + p))
            else
                L_eps = L
            end

            # interpolation constraint
            # ------------------------
            for i in I_N_star, j in I_N_star
                if i != j


                    @constraint(model_primal_PEP_with_known_stepsizes, Ft' * a_vec(i, j, ðŸ) + tr(G * A_mat(i, j, Î±, ð , ð±)) + ((1 / (2 * (L_eps))) * tr(G * C_mat(i, j, ð ))) - Îµ / 2 <= 0)
                end
            end
        end

    elseif sparsity_pattern == "OGM" 
        for m in 1:length(Îµ_set)
            if p < 1
                L_eps = ((1 - p) / (1 + p) * 1 / Îµ_set[m])^((1 - p) / (1 + p)) * (L)^(2 / (1 + p))
            else
                L_eps = L
            end

            # interpolation constraint
            # ------------------------
            for i in 0:N-1
                
                j = i+1 

                @constraint(model_primal_PEP_with_known_stepsizes, Ft' * a_vec(i, j, ðŸ) + tr(G * A_mat(i, j, Î±, ð , ð±)) + ((1 / (2 * (L_eps))) * tr(G * C_mat(i, j, ð ))) - Îµ_set[m] / 2 <= 0)
                                

            end

            for j in 0:N

                i = -1

                @constraint(model_primal_PEP_with_known_stepsizes, Ft' * a_vec(i, j, ðŸ) + tr(G * A_mat(i, j, Î±, ð , ð±)) + ((1 / (2 * (L_eps))) * tr(G * C_mat(i, j, ð ))) - Îµ_set[m] / 2 <= 0)

            end
        end


    end

    # initial condition
    # -----------------

    @constraint(model_primal_PEP_with_known_stepsizes, tr(G * B_mat(0, -1, Î±, ð±)) <= R^2)

    # time to optimize
    # ----------------

    if show_output == :off
        set_silent(model_primal_PEP_with_known_stepsizes)
    end

    optimize!(model_primal_PEP_with_known_stepsizes)

    # store and return the solution
    # -----------------------------

    if termination_status(model_primal_PEP_with_known_stepsizes) != MOI.OPTIMAL
        #   @warn "model_primal_PEP_with_known_stepsizes solving did not reach optimality;  termination status = " termination_status(model_primal_PEP_with_known_stepsizes)
    end

    p_star = objective_value(model_primal_PEP_with_known_stepsizes)

    G_star = value.(G)

    Ft_star = value.(Ft)

    return p_star, G_star, Ft_star

end


function solve_dual_PEP_with_known_stepsizes(N, L, Î±, R, Îµ_set, p, zero_idx;
    show_output=:off,
    Ïµ_tol_feas=1e-6,
    objective_type=:default,
    obj_val_upper_bound=default_obj_val_upper_bound)
    M = length(Îµ_set)
    # data generator
    # --------------

    ð±, ð , ðŸ = data_generator_function(N, L, Î±; input_type=:stepsize_constant)

    # Number of points etc
    # --------------------

    I_N_star = -1:N
    dim_Z = N + 2

    # define the model
    # ----------------

    model_dual_PEP_with_known_stepsizes = Model(optimizer_with_attributes(Mosek.Optimizer))

    # define the variables
    # --------------------

    # define the index set of Î»
    idx_set_Î» = index_set_constructor_for_dual_vars_full(I_N_star, M)

    @variable(model_dual_PEP_with_known_stepsizes, Î»[idx_set_Î»] >= 0)

    # define Î½

    @variable(model_dual_PEP_with_known_stepsizes, Î½ >= 0)

    # define Z âª° 0
    @variable(model_dual_PEP_with_known_stepsizes, Z[1:dim_Z, 1:dim_Z], PSD)




    if objective_type == :default

        @info "[ðŸ’ ] Minimizing the usual performance measure"

        @objective(model_dual_PEP_with_known_stepsizes, Min, Î½ * R^2 + sum(Î»[i_j_m_Î»]*Îµ_set[i_j_m_Î».m]/2 for i_j_m_Î» in idx_set_Î»))


    end
  
    # add the linear constraint
    # -------------------------

    @constraint(model_dual_PEP_with_known_stepsizes, sum(Î»[i_j_m_Î»] * a_vec(i_j_m_Î».i, i_j_m_Î».j, ðŸ) for i_j_m_Î» in idx_set_Î») - a_vec(-1, N, ðŸ) .== 0)


    # add the LMI constraint
    # ----------------------

    function L_Îµ(Îµ, p)
        return ((1 - p) / (1 + p) * 1 / Îµ)^((1 - p) / (1 + p)) * (L)^(2 / (1 + p))
    end






        @constraint(model_dual_PEP_with_known_stepsizes,
            Î½ * B_mat(0, -1, Î±, ð±) + sum(Î»[i_j_m_Î»] * (A_mat(i_j_m_Î».i, i_j_m_Î».j, Î±, ð , ð±)) for i_j_m_Î» in idx_set_Î») +
             sum((1 / (2 * (L_Îµ(Îµ_set[i_j_m_Î».m], p)))) * Î»[i_j_m_Î»] * C_mat(i_j_m_Î».i, i_j_m_Î».j, ð ) for i_j_m_Î» in idx_set_Î») 
          .==
            Z
        )


  

    for idx in zero_idx

        @constraint(model_dual_PEP_with_known_stepsizes, Î»[i_j_m_idx(idx...)] == 0)
    end
    # time to optimize
    # ----------------

    if show_output == :off
        set_silent(model_dual_PEP_with_known_stepsizes)
    end

    optimize!(model_dual_PEP_with_known_stepsizes)

    if termination_status(model_dual_PEP_with_known_stepsizes) != MOI.OPTIMAL
        @info "ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€ðŸ’€"
        @error "model_dual_PEP_with_known_stepsizes solving did not reach optimality;  termination status = " termination_status(model_dual_PEP_with_known_stepsizes)
    end

    # store the solutions and return
    # ------------------------------

    # store Î»_opt

    Î»_opt = value.(Î»)

    # store Î½_opt

    Î½_opt = value.(Î½)

    # store Z_opt

    #  Z_opt1 = value.(Z_1)
    #  Z_opt2 = value.(Z_2)

    # compute cholesky

    #   L_cholesky_opt =  compute_pivoted_cholesky_L_mat(Z_opt1)

    #   if norm(Z_opt1 - L_cholesky_opt*L_cholesky_opt', Inf) > 1e-6
    #      @info "checking the norm bound"
    #       @warn "||Z - L*L^T|| = $(norm(Z_opt - L_cholesky_opt*L_cholesky_opt', Inf))"
    #  end

    # effective index sets for the dual variables Î», Î¼, Î½

    # idx_set_Î»_effective = effective_index_set_finder(Î»_opt ; Ïµ_tol = 0.0005)

    # store objective

    #  â„“_1_norm_Î» = sum(Î»_opt)
    #   tr_Z = tr(Z_opt1)
    original_performance_measure = Î½_opt * R^2 + sum(Î»_opt[i_j_m_Î»]*Îµ_set[i_j_m_Î».m]/2 for i_j_m_Î» in idx_set_Î»)
    # return all the stored values

    return original_performance_measure, Î»_opt

end




##### Aaron's Code ######

struct OptimizationProblem
    N::Int
    L::Float64
    R::Float64
    p::Float64
    k::Int
end


function get_H(N, M, args)
    H = OffsetArray(zeros(N, N), 1:N, 0:N-1)
    cnt = 0
    for i in 1:N
        for j in 0:i-1
            H[i, j] = args[M+1+cnt]
            cnt += 1
        end
    end
    return H
end


function compute_Î±_from_h(prob::OptimizationProblem, h, Î¼)
    N, L = prob.N, prob.L
    Î± = OffsetArray(zeros(N, N), 1:N, 0:N-1)
    for â„“ in 1:N
        for i in 0:â„“-1
            if i == â„“ - 1
                Î±[â„“, i] = h[â„“, â„“-1]
            elseif i <= â„“ - 2
                Î±[â„“, i] = Î±[â„“-1, i] + h[â„“, i] - (Î¼ / L) * sum(h[â„“, j] * Î±[j, i] for j in i+1:â„“-1)
            end
        end
    end
    return Î±
end


function get_Î»_matrices(Î»_opt, N, M, TOL)
    Î»_matrices = zeros(N + 2, N + 2, M)
    for m = 1:M
        for i in -1:N
            for j in -1:N
                if i == j
                    continue
                end
                if Î»_opt[i_j_m_idx(i,j,m)] > TOL
                Î»_matrices[i+2,j+2,m] = Î»_opt[i_j_m_idx(i,j,m)]
                end
            end
        end
    end
    return Î»_matrices
end

function run_batch(prob::OptimizationProblem, args)
    N, L, R, p, k = prob.N, prob.L, prob.R, prob.p, prob.k
    
    H = OffsetArray(zeros(N, N), 1:N, 0:N-1)
    cnt = 0
    for i in 1:N
        for j in 0:i-1

            H[i, j] = args[k+1+cnt]
            cnt += 1
        end
    end
    Îµ_set = args[1:k]
    Î¼ = 0
    Î± = compute_Î±_from_h(prob, H, Î¼)
    Y, _, _ = solve_primal_with_known_stepsizes_batch(N, L, Î±, R, Îµ_set, p, Î¼; show_output=:off)
    return Y
end


function optimize_Îµ_h(N, L, R, p, k, max_iter, max_time, lower, upper, initial_vals)

    prob = OptimizationProblem(N, L, R, p, k)
   
    terminated_early = false

    run_batch(prob, initial_vals)

    iter_print_freq = 100

    function my_callback(state)

        if state.iteration % iter_print_freq == 0
            println("Iter $(state.iteration): f = $(state.value)")
        end
        if state.value < 0
            println("Terminating early: negative objective detected (f = $(state.value))")
            terminated_early = true

            return terminated_early
        end
        return false
    end

    options = Optim.Options(
        iterations=max_iter,
        f_tol= 1e-8,
        x_tol= 1e-8,
        time_limit=max_time,
        show_trace=false,
        callback=my_callback
    )

    result = Optim.optimize(args -> run_batch(prob, args), lower, upper, initial_vals, Fminbox(NelderMead()), options)
    return result, terminated_early
end



function Optimize(N, L, R, p, M, initial_vals)
    T = 0
    max_iter = 1500
    max_time = 30
    H_size = div(N * (N + 1), 2)
    lower = 0.00000001 * [ones(M); ones(H_size)]
    upper = 2 * [ones(M); ones(H_size)]
   # prob = OptimizationProblem(N, L, R, p, M)
    result, terminated_early = optimize_Îµ_h(N, L, R, p, M, max_iter, max_time, lower, upper, initial_vals)
    minimizer = Optim.minimizer(result)
    H = get_H(N, M, minimizer)

    Îµ_set = minimizer[1:M]
    return Îµ_set, H, result, terminated_early

end

 
function run_batch_trials(N, L, R, p, M, trials)

    min_F = Inf
    min_H = nothing
    min_Îµ = nothing
    H_size = div(N * (N + 1), 2)
    OGM = get_OGM_vector(N)
    for i in 1:trials
        initial_vals = [0.00002 * rand(M) .+ 0.0002; OGM ./ (1.5 .+ 0.1 * rand(H_size))]
        Îµ_set, H, result, terminated_early = Optimize(N, L, R, p, M, initial_vals)
        T = Optim.f_calls(result)
        println("Trial $i done")
        if !terminated_early
            println(Îµ_set)
            display(H)
            if Optim.minimum(result) < min_F
                min_F = Optim.minimum(result)
                min_H = H
                min_Îµ = Îµ_set
            end
        end
    end

    return min_F, min_H, min_Îµ
end



function compute_theta(N)  ## Ben's Code
    Î¸ = zeros(N + 1)
    Î¸[1] = 1.0  # Î¸â‚€
    for i in 2:N
        Î¸[i] = (1 + sqrt(1 + 4 * Î¸[i-1]^2)) / 2
    end
    Î¸[N+1] = (1 + sqrt(1 + 8 * Î¸[N]^2)) / 2  # Î¸_N
    return Î¸
end
function compute_H(N)
    Î¸ = compute_theta(N)
    H = zeros(N, N)
    for i in 1:N
        for k in 1:i-1
            sum_hjk = sum(H[j, k] for j in k:i)
            H[i, k] = (1 / Î¸[i+1]) * (2 * Î¸[k] - sum_hjk)
        end
        H[i, i] = 1 + (2 * Î¸[i] - 1) / Î¸[i+1]
    end
    return H
end



function get_OGM_vector(N)
    OGM = compute_H(N)
    vec_lower = zeros(div(N * (N + 1), 2))
    cnt = 0
    for i in 1:N
        for j in 1:i
            cnt = cnt + 1
            vec_lower[cnt] = OGM[i, j]
        end
    end
    return vec_lower
end


function get_vector(N,H)
    vec_lower = zeros(div(N * (N + 1), 2))
    cnt = 0
    for i in 1:N
        for j in 1:i
            cnt = cnt + 1
            vec_lower[cnt] = H[i, j]
        end
    end
    return vec_lower
end


function gen_data(L, R, trials, p_cnt, N_cnt, M_cnt)

    results = Dict{Int, Dict{Int, Dict{String, Any}}}()

    for N = 1:N_cnt
        results[N] = Dict{Int, Dict{String, Any}}()

        for M = 1:N+M_cnt  # change back to just 1:M_cnt
            Îµ_p_data = Dict{Float64, Any}()
            F_p_data = Dict{Float64, Any}()
            H_p_data = Dict{Float64, Any}()

            for p in LinRange(0.9, 1, p_cnt)
                min_F, min_H, min_Îµ = run_batch_trials(N, L, R, p, M, trials)

                Îµ_p_data[p] = min_Îµ
                F_p_data[p] = min_F
                H_p_data[p] = min_H
            end

            results[N][M] = Dict(
                "F_values" => F_p_data,
                "Îµ_sets" => Îµ_p_data,
                "H_matrices" => H_p_data
            )
        end
    end
    return results
end

